# -*- coding: utf-8 -*-
"""HW5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xBGy6Q7SOn-hpKqUHorsw94SlUfPYxsA

# HW5 RAG

## 1. è«‹å®Œæˆä»¥ä¸‹ç¨‹å¼ (50%)

ä»¥ä¸‹ç¨‹å¼ç¢¼æœ‰è¨±å¤šä¸å®Œæ•´çš„åœ°æ–¹ï¼Œè«‹ä¾ç…§åŸæœ¬çš„ç¨‹å¼æ¡†æ¶ï¼Œè£œé½Šæ‰€æœ‰æœªå®Œæˆçš„éƒ¨åˆ†ï¼Œè«‹ç¢ºä¿æ‰€æœ‰ç¨‹å¼ç¢¼èƒ½å®Œæ•´åŸ·è¡Œä¸¦å¾—åˆ°åˆç†çš„å›ç­”çµæœã€‚
"""

# âœ… STEP 1: å®‰è£éœ€è¦çš„å¥—ä»¶

!pip install -q sentence-transformers faiss-cpu requests

# âœ… STEP 2: è¼‰å…¥å¿…è¦æ¨¡çµ„

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import requests
import pandas as pd

### éå°å¤§å­¸ç”Ÿè«‹åŸ·è¡Œä»¥ä¸‹ç¨‹å¼ç¢¼ç²å–èªæ–™ ###

!gdown 1AF2g2WTtXQwb02S5EAD7aL48c9BkV1ed

### éå°å¤§å­¸ç”Ÿè«‹åŸ·è¡Œä»¥ä¸Šç¨‹å¼ç¢¼ç²å–èªæ–™ ###

# âœ… STEP 3: æº–å‚™ä½ çš„èªæ–™

docs_df = ("dcard-top100.csv")
df = pd.read_csv(docs_df)
documents = df.content.tolist()
docs_df

# âœ… STEP 4: å»ºç«‹ embedding æ¨¡å‹èˆ‡ FAISS ç´¢å¼•

embedding_model = SentenceTransformer("shibing624/text2vec-base-multilingual")

# è½‰æ›ç‚ºå‘é‡ä¸¦æ­£è¦åŒ–ï¼ˆä¾¿æ–¼ cosine ç›¸ä¼¼åº¦ï¼‰
doc_embeddings = embedding_model.encode(documents, normalize_embeddings=True)

# å»ºç«‹å‘é‡ç´¢å¼•
index = faiss.IndexFlatIP(doc_embeddings.shape[1])
index.add(doc_embeddings)

from google.colab import userdata
HUGGINGFACE_TOKEN = userdata.get('Hugging_face')

# ğŸ¤– STEP 5: å‘¼å« HuggingFace æ¨¡å‹ API

import requests
import os
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")


API_URL = "https://api-inference.huggingface.co/models/ClueAI/ChatYuan-large-v2"

headers = {"Authorization": f"Bearer {HUGGINGFACE_TOKEN}"}

# è«‹å®Œæˆ prompt
def generate_answer(query, context_docs):
    context = "\n".join(f"- {doc[:300]}" for doc in context_docs)  # é¿å…å¤ªé•·
    prompt = f"""ä½ æ˜¯ä¸€å€‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼Œè‹¥æ²’æœ‰ç›¸é—œè³‡è¨Šè«‹å›ç­”ã€Œæ‰¾ä¸åˆ°ç­”æ¡ˆã€ã€‚

è³‡æ–™ï¼š{context}

å•é¡Œï¼š{query}

è«‹ç”¨ä¸€åˆ°å…©å¥è©±å›ç­”ã€‚
"""

    payload = {"inputs": prompt}
    response = requests.post(API_URL, headers=headers, json=payload, timeout=60)

    try:
        data = response.json()
        if isinstance(data, list) and "generated_text" in data[0]:
            return data[0]["generated_text"]
        elif isinstance(data, dict) and "generated_text" in data:
            return data["generated_text"]
        else:
            return f"âš ï¸ ç„¡æ³•è§£ææ¨¡å‹å›æ‡‰ï¼š{data}"
    except Exception as e:
        return f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

def search(query, top_k=3):

  query_vec = embedding_model.encode([query], normalize_embeddings=True)
  scores, indices = index.search(query_vec, top_k)
  matched_docs = [documents[i] for i in indices[0]]

  return matched_docs

# âœ… STEP 6: ä½¿ç”¨è€…è¼¸å…¥å•é¡Œï¼Œæ‰¾å‡ºæœ€ç›¸é—œçš„å…§å®¹

query = "åœ¨å“ªè£¡å¯ä»¥çœ‹åˆ°å¾ˆå¤šè‰è“ï¼Ÿ"

relevant_docs = search(query, top_k=3)
answer = generate_answer(query, relevant_docs)

print("ğŸ” ç›¸é—œæ®µè½ï¼š\n", "\n---\n".join(relevant_docs)[:100])
print("\nğŸ’¬ AI å›ç­”ï¼š\n", answer)

"""## 2. è«‹å›ç­”ä»¥ä¸‹å•é¡Œ (50%)

è«‹å˜—è©¦è¼¸å…¥è‡³å°‘ 3 å€‹å•é¡Œçµ¦ä½ çš„å•ç­”ç³»çµ±ï¼Œä¸¦è§€å¯Ÿæ¨¡å‹å›ç­”çš„æº–ç¢ºæ€§èˆ‡è¡¨ç¾ï¼š

- ä½ å•äº†å“ªäº›å•é¡Œï¼Ÿ

- æ¨¡å‹æ˜¯å¦æˆåŠŸå¾èªæ–™ä¸­æ‰¾åˆ°èˆ‡å•é¡Œç›¸é—œçš„å…§å®¹ï¼Ÿ

- å›ç­”æ˜¯å¦åˆç†ï¼Ÿæ˜¯å¦æœ‰äº‚ç·¨çš„æƒ…æ³ï¼Ÿ

- è‹¥å•é¡Œç„¡æ³•å›ç­”ï¼Œæ¨¡å‹æ˜¯å¦èƒ½æ­£ç¢ºå›æ‡‰ã€Œæ‰¾ä¸åˆ°ç­”æ¡ˆã€ï¼Ÿ

è«‹æ ¹æ“šä½ çš„è§€å¯Ÿï¼Œç°¡è¿°æ¨¡å‹æ•´é«”è¡¨ç¾çš„å„ªç¼ºé»ã€‚

"""